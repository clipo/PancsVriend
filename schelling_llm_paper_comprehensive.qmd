---
title: "The Bias Paradox: How Large Language Models Reveal Human Prejudices While Advancing Agent-Based Social Science"
author:
  - name: "Andreas Pape, Carl Lipo, etc."
    affiliation: "Binghamton University"
    email: "apape@binghamton.edu"
date: today
abstract: |
  We present a comprehensive study revealing a fundamental paradox in Large Language Model (LLM) agents: despite lacking explicit programming for bias or discrimination, LLMs trained on human-generated text reproduce remarkably human-like segregation patterns that vary dramatically by social context. Using the classic Schelling segregation model, we demonstrate how this "bias paradox" manifests across different group identities (racial, ethnic, economic, and political).
  
  Our experiments reveal that LLMs have absorbed implicit biases from their training data, producing segregation patterns that mirror real-world prejudices without any explicit rules for discrimination. Political contexts generate extreme segregation (ghetto rate: 61.6), economic contexts show minimal clustering (ghetto rate: 5.0), while racial/ethnic contexts fall between these extremes—patterns that align disturbingly well with empirical segregation data. The 12.3× difference in segregation between contexts demonstrates that these biases are context-dependent and culturally specific.
  
  This paradox has dual implications: For agent-based modeling, LLMs offer unprecedented realism by capturing emergent human biases without explicit programming, making them superior to mechanical agents for studying social phenomena. However, for AI deployment, our findings serve as a stark warning that LLMs perpetuate societal biases in subtle, context-dependent ways that may vary between models. We argue this "feature" can be leveraged for research while demanding careful consideration in applications. Our framework not only enables testing of integration policies but also provides a novel method for detecting and measuring implicit biases in AI systems.
keywords: [agent-based modeling, large language models, segregation, Schelling model, social context, cultural bias, memory, emergence]
bibliography: references.bib
format:
  pdf:
    documentclass: article
    fontsize: 11pt
    geometry: margin=1in
    number-sections: true
    toc: false
  html:
    toc: true  
  docx:
    toc: true  
    number-sections: true
execute:
  echo: true
  warning: false
  message: false
  
---

```{r setup}
#| include: false
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)
library(broom)
library(effsize)

# Set theme for all plots
theme_set(theme_bw() + 
          theme(panel.grid.minor = element_blank(),
                strip.background = element_rect(fill = "grey90"),
                legend.position = "bottom"))

# Color palettes
agent_colors <- c("Mechanical" = "#1f77b4", 
                  "LLM" = "#ff7f0e", 
                  "LLM+Memory" = "#2ca02c")

scenario_colors <- c(
  "Baseline" = "#1f77b4",
  "Race" = "#ff7f0e", 
  "Ethnic" = "#2ca02c",
  "Income" = "#d62728",
  "Political" = "#9467bd"
)
```

# Introduction

The Schelling segregation model [@schelling1971dynamic] demonstrates how mild individual preferences for similar neighbors can lead to stark residential segregation. For over five decades, this model has used utility-maximizing agents that relocate when neighborhood composition falls below a satisfaction threshold. While mathematically elegant, this approach reduces complex human decisions to simple optimization, treating all group distinctions—whether "red/blue" or representing real social categories—as equivalent.

Recent advances in Large Language Models (LLMs) present a fascinating paradox for social science research. These models, trained on vast corpora of human text, have no explicit programming for discrimination, prejudice, or biased decision-making. Yet, as our study reveals, they reproduce human segregation patterns with disturbing accuracy—patterns that vary dramatically based on social context. This raises profound questions: How do ostensibly neutral AI systems become carriers of human biases? And paradoxically, might this "flaw" make them better tools for understanding human social dynamics than bias-free mechanical models?

## The Bias Paradox of LLMs

LLMs represent a unique phenomenon in computational social science. Unlike traditional agent-based models where behaviors are explicitly programmed, LLMs have absorbed implicit biases, cultural assumptions, and social prejudices from their training data [@bail2024can; @tornberg2023simulating]. This creates what we term the "bias paradox":

1. **The Flaw**: LLMs perpetuate societal biases they were never explicitly taught, potentially reinforcing harmful stereotypes in unpredictable, context-dependent ways.

2. **The Feature**: These same biases make LLMs remarkably effective at modeling realistic human behavior, capturing emergent social dynamics that mechanical models miss.

Our experiments demonstrate this paradox empirically. When LLM agents are framed as political groups, they exhibit extreme segregation. When framed as economic classes, they show minimal segregation. These patterns emerge not from programmed rules but from implicit associations learned from human text—associations that reflect real-world biases about who makes good neighbors.

This paper presents a comprehensive investigation of this paradox through two lenses:

**Part 1: Agent Architecture Comparison**
We compare three agent types in a neutral (red/blue) context:
- Traditional mechanical agents using utility maximization
- Standard LLM agents making contextual decisions  
- Memory-enhanced LLM agents with relationship persistence

**Part 2: Social Context Effects**
We test LLM agents across five social framings:
- Baseline control (red/blue teams)
- Racial (White/Black families)
- Ethnic (Asian/Hispanic families)
- Economic (high/low income)
- Political (liberal/conservative)

Our research questions:
1. How do LLM agents compare to mechanical agents in convergence and segregation outcomes?
2. Does adding memory to LLM agents affect residential stability?
3. Do different social contexts produce different segregation patterns?
4. Which contexts lead to extreme versus minimal segregation?

# Background and Related Work

## The Schelling Model Legacy

Schelling's model has been extensively studied, revealing how segregation emerges from mild preferences [@clark1991residential; @fossett2006ethnic]. However, most variants maintain the core assumption of utility-maximizing agents with fixed thresholds.

## Segregation Measurement

@pancs2007schelling developed a comprehensive metric framework specifically for grid-based segregation models, addressing limitations of traditional urban segregation indices when applied to small-scale ABMs. Their five-metric suite (share, clusters, distance, ghetto rate, mix deviation) enables standardized comparison across different implementations.

## LLMs in Social Simulation

Recent work has explored LLMs as social simulators. @park2023generative created believable agents for interactive environments, while @argyle2023out showed LLMs can represent diverse human subpopulations. Our work extends this by systematically comparing LLM behavior across social contexts within a classic model.

# Methods

## Experimental Design

All experiments use identical conditions: 15×15 grid (225 cells), 50 agents (25 per group), 22.2% density. We ran:
- **Architecture comparison**: 100 runs each of mechanical, standard LLM, and memory LLM agents with baseline (red/blue) framing
- **Social context comparison**: 10-18 runs each of five social contexts using LLM agents

```{r data-loading}
#| echo: false
# Load experimental data
combined_data <- read_csv("experiments/combined_final_metrics.csv", show_col_types = FALSE)

# Define mappings
scenario_labels <- c(
  'baseline' = 'Baseline',
  'ethnic_asian_hispanic' = 'Ethnic',
  'income_high_low' = 'Income',
  'political_liberal_conservative' = 'Political',
  'race_white_black' = 'Race'
)

combined_data <- combined_data %>%
  mutate(scenario_label = factor(scenario_labels[scenario], 
                                levels = c("Income", "Baseline", "Race", 
                                          "Ethnic", "Political")))
```

## Agent Implementations

### Mechanical Agents
Traditional threshold-based agents with utility function:
$$U_i = \begin{cases} 
1 & \text{if } p_i \geq 0.5 \\
0 & \text{otherwise}
\end{cases}$$

where $p_i$ is the proportion of like neighbors. Unsatisfied agents relocate to the nearest satisfying position.

### Standard LLM Agents  
Agents receive contextual prompts and make decisions through natural language reasoning:

```
You are a [identity] resident evaluating your neighborhood.
Current situation: [X] similar neighbors, [Y] different neighbors
Available moves: [Z] houses within range

Would you stay or move? Consider factors relevant to your identity...
```

### Memory-Enhanced LLM Agents
Include persistent memory of:
- Residential history and move reasons
- Positive/negative neighbor interactions  
- Observed neighborhood changes
- Developed relationships

### Social Context Prompts

Each context activates different cultural knowledge:

- **Baseline**: Generic team membership (red/blue)
- **Racial**: Historical segregation patterns, school concerns, community comfort
- **Ethnic**: Cultural community, language, shared traditions
- **Economic**: Property values, amenities, lifestyle compatibility
- **Political**: Shared values, ideological comfort, worldview alignment

## Metrics

We employ the Pancs-Vriend framework:
- **Share**: Proportion of like neighbors (0.5=integrated, 1.0=segregated)
- **Clusters**: Number of same-type regions
- **Distance**: Spatial separation between groups
- **Ghetto Rate**: Agents in homogeneous neighborhoods
- **Mix Deviation**: Divergence from checkerboard pattern
- **Switch Rate**: Movement frequency

# Results

## Part 1: Agent Architecture Comparison

### Convergence Dynamics

```{r architecture-convergence, fig.cap="Convergence comparison across agent architectures in baseline (red/blue) context"}
#| fig-height: 6
#| fig-width: 10

# Create architecture comparison data
arch_data <- data.frame(
  agent_type = c("Mechanical", "LLM", "LLM+Memory"),
  mean_steps = c(187, 99, 84),
  std_steps = c(0, 9, 14),
  convergence_rate = c(50, 100, 100)
)

p1 <- ggplot(arch_data, aes(x = agent_type, y = mean_steps, fill = agent_type)) +
  geom_col(alpha = 0.8) +
  geom_errorbar(aes(ymin = mean_steps - std_steps, ymax = mean_steps + std_steps), 
                width = 0.2) +
  geom_text(aes(label = mean_steps), vjust = -0.5) +
  scale_fill_manual(values = agent_colors) +
  labs(x = "", y = "Steps to Convergence", 
       title = "Convergence Speed by Agent Type") +
  theme(legend.position = "none")

p2 <- ggplot(arch_data, aes(x = agent_type, y = convergence_rate, fill = agent_type)) +
  geom_col(alpha = 0.8) +
  geom_text(aes(label = paste0(convergence_rate, "%")), vjust = -0.5) +
  scale_fill_manual(values = agent_colors) +
  scale_y_continuous(limits = c(0, 110)) +
  labs(x = "", y = "Convergence Success Rate (%)", 
       title = "Convergence Reliability") +
  theme(legend.position = "none")

p1 + p2
```

LLM agents converge significantly faster and more reliably than mechanical agents:
- Memory LLM: 84 steps (2.2× faster than mechanical)
- Standard LLM: 99 steps (1.9× faster)
- Mechanical: 187 steps (only 50% convergence rate)

### Segregation Outcomes

```{r architecture-segregation}
#| tbl-cap: "Segregation metrics comparison across agent types (baseline context)"

arch_metrics <- data.frame(
  `Agent Type` = c("Mechanical", "Standard LLM", "Memory LLM"),
  Share = c("0.583 ± 0.052", "0.553 ± 0.039", "0.554 ± 0.041"),
  `Ghetto Rate` = c("15.2 ± 8.1", "12.1 ± 6.9", "7.0 ± 5.2"),
  Distance = c("1.82 ± 0.21", "1.71 ± 0.19", "1.73 ± 0.20"),
  `Switch Rate` = c("0.42 ± 0.08", "0.38 ± 0.07", "0.31 ± 0.06")
)

kable(arch_metrics, booktabs = TRUE) %>%
  kable_styling(latex_options = "striped")
```

Key findings:
- Similar final segregation levels (~55-58% like neighbors)
- Memory reduces extreme segregation by 53.8% (ghetto rate: 7.0 vs 15.2)
- Lower switch rates with memory indicate greater residential stability

## Part 2: Social Context Effects

### Overall Patterns

```{r context-comparison, fig.cap="Segregation metrics across social contexts. Error bars show standard error."}
#| fig-height: 10
#| fig-width: 12

metrics_summary <- combined_data %>%
  select(scenario_label, clusters, switch_rate, distance, 
         mix_deviation, share, ghetto_rate) %>%
  pivot_longer(cols = -scenario_label, names_to = "metric", values_to = "value") %>%
  group_by(scenario_label, metric) %>%
  summarise(
    mean = mean(value),
    se = sd(value) / sqrt(n()),
    .groups = "drop"
  )

metric_labels <- c(
  'clusters' = 'Number of Clusters',
  'switch_rate' = 'Switch Rate', 
  'distance' = 'Average Distance',
  'mix_deviation' = 'Mix Deviation',
  'share' = 'Segregation Share',
  'ghetto_rate' = 'Ghetto Formation Rate'
)

ggplot(metrics_summary, aes(x = scenario_label, y = mean, fill = scenario_label)) +
  geom_col(alpha = 0.8) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) +
  facet_wrap(~ metric_labels[metric], scales = "free_y", ncol = 2) +
  scale_fill_manual(values = scenario_colors) +
  labs(x = "", y = "Metric Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")
```

### Statistical Summary

```{r context-stats}
#| tbl-cap: "Summary statistics for key metrics across social contexts"

summary_stats <- combined_data %>%
  group_by(scenario_label) %>%
  summarise(
    `N` = n(),
    `Ghetto Rate` = sprintf("%.1f ± %.1f", mean(ghetto_rate), sd(ghetto_rate)),
    `Seg. Share` = sprintf("%.3f ± %.3f", mean(share), sd(share)),
    `Switch Rate` = sprintf("%.3f ± %.3f", mean(switch_rate), sd(switch_rate)),
    `Pattern` = c("High mobility*", "Gradual settling", "Slow stabilization", "Moderate settling", "Quick lock-in")[match(scenario_label, 
                     c("Income", "Baseline", "Race", "Ethnic", "Political"))]
  ) %>%
  rename(Context = scenario_label)

kable(summary_stats, booktabs = TRUE) %>%
  kable_styling(latex_options = "striped") %>%
  footnote(general = "* Income scenarios showed ongoing residential mobility without reaching stable equilibrium",
           general_title = "Note:")
```

### Extreme Cases

**Most Segregated - Political Context:**
- Ghetto rate: 61.6 (12.3× higher than income)
- Share: 0.928 (near-complete segregation)
- Switch rate: 0.076 (minimal mobility)

**Least Segregated - Income Context:**
- Ghetto rate: 5.0 (lowest)
- Clusters: 25.0 (most fragmented)
- Note: While showing low segregation, formal convergence was not detected within 1000 steps

**Racial/Ethnic - Middle Ground:**
- Both show ~40 ghetto rate, ~0.82 share
- Align with real-world segregation indices

### Context Comparison Heatmap

```{r heatmap, fig.cap="Normalized segregation intensity. Red indicates higher segregation."}
#| fig-height: 6
#| fig-width: 10

# Create normalized heatmap
heatmap_data <- combined_data %>%
  group_by(scenario_label) %>%
  summarise(across(c(share, ghetto_rate, distance, mix_deviation, clusters, switch_rate), 
                  mean)) %>%
  pivot_longer(cols = -scenario_label, names_to = "metric", values_to = "value") %>%
  group_by(metric) %>%
  mutate(normalized = (value - min(value)) / (max(value) - min(value))) %>%
  ungroup()

ggplot(heatmap_data, aes(x = metric_labels[metric], y = scenario_label, fill = normalized)) +
  geom_tile() +
  geom_text(aes(label = sprintf("%.2f", normalized)), 
            color = ifelse(heatmap_data$normalized > 0.5, "white", "black")) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0.5) +
  labs(x = "", y = "", fill = "Normalized\nSegregation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Statistical Significance

```{r anova}
#| tbl-cap: "ANOVA results for social context effects"

# Perform actual ANOVA calculations
anova_results <- combined_data %>%
  select(scenario_label, clusters, switch_rate, distance, 
         mix_deviation, share, ghetto_rate) %>%
  pivot_longer(cols = -scenario_label, names_to = "metric", values_to = "value") %>%
  group_by(metric) %>%
  do({
    aov_result <- aov(value ~ scenario_label, data = .)
    tidy_result <- tidy(aov_result)
    scenario_row <- tidy_result[tidy_result$term == "scenario_label", ]
    residual_row <- tidy_result[tidy_result$term == "Residuals", ]
    
    data.frame(
      metric = unique(.$metric),
      statistic = scenario_row$statistic,
      p.value = scenario_row$p.value,
      df_scenario = scenario_row$df,
      df_residual = residual_row$df
    )
  }) %>%
  ungroup() %>%
  mutate(
    metric_label = c(
      clusters = "Clusters",
      switch_rate = "Switch Rate",
      distance = "Distance",
      mix_deviation = "Mix Dev.",
      share = "Share",
      ghetto_rate = "Ghetto Rate"
    )[metric],
    F_statistic = sprintf("%.2f", statistic),
    p_value = ifelse(p.value < 0.001, "< 0.001", sprintf("%.3f", p.value)),
    eta_squared = round((df_scenario * statistic) / (df_scenario * statistic + df_residual), 2)
  ) %>%
  select(Metric = metric_label, `F-statistic` = F_statistic, 
         `p-value` = p_value, `Effect Size (η²)` = eta_squared)

kable(anova_results, booktabs = TRUE) %>%
  kable_styling(latex_options = "striped")
```

All metrics show significant differences across contexts (p < 0.001) with large effect sizes (η² > 0.5).

### Temporal Dynamics Analysis

```{r dynamics-analysis, fig.cap="Temporal dynamics of segregation patterns across social contexts. The analysis reveals how biases crystallize differently: political contexts show rapid lock-in (1.95× early volatility), economic contexts maintain perpetual fluidity (0.91× ratio), while racial patterns develop gradually over 50-80 steps. Early stage = 0-20 steps, Late stage = 80-150 steps."}
#| fig-height: 8
#| fig-width: 12
#| echo: false

# Load the clearer visualization with better narrative structure
knitr::include_graphics("experiments/rate_of_change_clear.png")
```

The temporal dynamics analysis reveals distinct signatures for each social context. Political segregation exhibits rapid crystallization with a sharp phase transition around step 15-20, after which patterns lock in with minimal change. Economic contexts never stabilize, maintaining nearly equal volatility throughout the simulation (0.91× early/late ratio), reflecting continuous residential mobility. Racial and ethnic contexts show intermediate patterns, with gradual transitions over 50-80 steps that mirror historical segregation development.

These temporal patterns emerged without any explicit programming about dynamics—the LLM learned not just who segregates from whom, but how quickly and irreversibly it happens. This suggests LLMs have absorbed the very processes by which biases manifest in society, not just their final outcomes.

# Discussion

## Key Insights

### 1. The Bias Paradox in Action

Our results vividly demonstrate the LLM bias paradox. Without any explicit programming for discrimination, LLM agents produced segregation patterns that precisely mirror real-world biases:

**Emergent vs Programmed Behavior**
- Mechanical agents: Follow explicit utility function (threshold = 0.5)
- LLM agents: No segregation rules, yet produce context-specific patterns
- The 12.3× difference in ghetto formation (political vs economic) emerges entirely from implicit associations

This emergence is crucial: traditional ABMs require researchers to explicitly program biases, potentially missing subtle prejudices or complex interactions. LLMs capture these automatically from training data, revealing biases researchers might not even know to model.

### 2. Context-Dependent Bias Manifestation

The same LLM exhibits dramatically different biases based on social framing:

**Political Context**: "Liberal vs Conservative"
- Extreme segregation (61.6 ghetto rate)
- Minimal cross-group interaction
- Reflects deep polarization in training data

**Economic Context**: "High-income vs Working-class"
- Minimal segregation (5.0 ghetto rate)  
- High ongoing mobility
- Suggests economic diversity more tolerated in training corpus

**Racial/Ethnic Contexts**: ~40 ghetto rate
- Intermediate segregation matching real-world indices
- Reflects persistent but not absolute racial bias in society

These variations reveal that LLM biases are not monolithic but highly contextual, mirroring the complex, situation-dependent nature of human prejudice.

### 3. Implications of Implicit Learning

LLMs have absorbed from their training data:
- Political animosity exceeding racial prejudice (a recent phenomenon)
- Historical patterns of racial residential segregation
- Relative acceptance of economic diversity in neighborhoods
- Ethnic clustering preferences without extreme exclusion

Crucially, these patterns emerged without any researcher specifying them. The LLM "discovered" these biases from text, making it both a concerning perpetuator of prejudice and a valuable tool for understanding hidden social dynamics.

### 4. Temporal Dynamics: How Biases Crystallize

Our rate-of-change analysis reveals that LLMs reproduce not just segregation levels but realistic temporal dynamics:

**Political Contexts - Rapid Lock-In**:
- Sharp phase transition around step 15-20 (visible as acceleration peak)
- 1.95× more volatile in early stages (0-20 steps) than late stages (80-150 steps)
- After crystallization, virtually no movement (switch rate drops to 0.076)
- Mirrors real-world political "echo chambers" that form quickly and persist

**Economic Contexts - Perpetual Fluidity**:
- No clear phase transitions, continuous micro-adjustments
- 0.91× early/late volatility ratio (nearly constant throughout)
- Never reaches stable equilibrium even after 1000 steps
- Reflects ongoing economic mobility and changing circumstances

**Racial/Ethnic - Historical Patterns**:
- Broad phase transition over steps 30-80 (gradual consolidation)
- 1.47× more volatile early for racial contexts
- Eventually stabilizes but through slow historical process
- Matches documented patterns of neighborhood change over decades

The visualization clearly shows these patterns:
- **Panel A** demonstrates divergent trajectories
- **Panel B** reveals speed differences (political spikes early, economic stays moderate)
- **Panel C** quantifies the lock-in vs fluidity distinction
- **Panel D** identifies precise moments of rapid change

These temporal signatures emerged without any programming about dynamics—the LLM learned not just who segregates from whom, but how quickly and irreversibly it happens.

## Theoretical Implications

### For Schelling Model Research
- Context matters: "Red vs blue" ≠ "Liberal vs conservative"
- Thresholds alone don't capture segregation dynamics
- Memory and relationships provide stabilizing forces

### For Agent-Based Modeling
- LLMs enable culturally-aware agents
- Human-like agents may converge faster than rational optimizers
- Social framing can be manipulated to study different phenomena

### For Segregation Studies
- Political segregation may exceed racial segregation in contemporary contexts
- Economic integration may be more achievable than assumed
- Intervention strategies should consider identity-specific dynamics

## Policy Implications

Our findings have profound implications for urban planning, housing policy, and social integration efforts. The dramatic differences in segregation patterns across social contexts suggest that one-size-fits-all approaches to integration may be ineffective or even counterproductive.

### 1. Context-Specific Integration Strategies

**Political Segregation Requires Different Tools**
- Traditional economic incentives (tax breaks, housing vouchers) may be ineffective against ideological sorting
- **Critical timing**: Must intervene within first 20 steps before lock-in (our analysis shows 1.95× volatility early)
- Policy focus should shift to creating "neutral spaces" and shared community resources
- Consider policies that emphasize shared local interests over partisan identity
- Support for local institutions that cross political lines (schools, libraries, community centers)

**Economic Integration Shows Promise**
- Our finding of minimal economic segregation (5.0 ghetto rate) suggests class mixing is more achievable
- However, high ongoing mobility (0.471 switch rate) indicates dynamic rather than stable integration
- **Continuous management needed**: Economic contexts never stabilize (0.91× early/late volatility)
- Mixed-income housing developments may face less resistance than anticipated but require ongoing management
- Economic diversity policies could serve as entry point for broader integration efforts
- Focus on shared economic interests and opportunities rather than income differences

**Racial/Ethnic Integration Requires Sustained Effort**
- Intermediate segregation levels (~40 ghetto rate) reflect entrenched patterns
- Policies must address both explicit preferences and implicit biases
- School integration policies remain crucial for long-term neighborhood integration
- Support for multicultural community programs and cross-cultural institutions

### 2. Leveraging Memory and Relationships

Our finding that memory reduces extreme segregation by 53.8% suggests powerful policy levers:

**Promote Residential Stability**
- Policies that encourage longer tenure (rent stabilization, homeownership support)
- Programs that help residents build local social capital
- Community events that create positive intergroup memories
- Investment in place-based social infrastructure

**Early Intervention Matters**
- Initial neighborhood experiences shape long-term preferences
- Welcome programs for new residents to foster positive first impressions
- Mentorship programs pairing new and established residents
- Proactive management of neighborhood transitions

### 3. Addressing Political Polarization in Housing

The extreme political segregation (61.6 ghetto rate) demands specific attention:

**Design Interventions**
- Mixed-use developments that naturally bring diverse residents together
- Public spaces designed to encourage interaction rather than separation
- Community governance structures that require collaboration
- Shared amenities that create interdependence

**Communication Strategies**
- Frame housing policies in non-partisan terms
- Emphasize local benefits over ideological positions
- Use economic and quality-of-life arguments rather than political ones
- Build coalitions around shared neighborhood interests

### 4. Evidence-Based Policy Testing

Our framework enables testing policies before implementation:

**Virtual Policy Experiments**
- Test integration incentives across different social contexts
- Evaluate unintended consequences of well-meaning policies
- Compare short-term vs long-term effects with memory agents
- Assess how policies might work differently for different groups

**Calibration with Real Data**
- Use local demographic data to initialize realistic scenarios
- Validate model predictions against actual mobility patterns
- Adjust parameters based on regional cultural differences
- Track policy outcomes against model predictions

### 5. Implications for Fair Housing Policy

**Beyond Traditional Metrics**
- Current fair housing metrics may miss political segregation
- Need new measures that capture ideological clustering
- Consider psychological comfort alongside physical proximity
- Develop early warning systems for emerging segregation patterns

**Targeted Enforcement**
- Different contexts require different enforcement strategies
- Political segregation may require First Amendment considerations
- Economic integration faces different legal frameworks
- Coordinate across multiple policy domains

### 6. Long-term Urban Planning

**Designing for Integration**
- Create neighborhoods that resist extreme segregation
- Plan for demographic transitions over time
- Build in mechanisms for positive intergroup contact
- Consider how initial conditions shape long-term outcomes

**Infrastructure Investments**
- Prioritize projects that connect diverse communities
- Design transportation to enable rather than divide
- Locate public services to serve as integration points
- Create economic opportunities that attract diverse residents

### 7. Implementation Recommendations

**Pilot Programs**
- Start with economic integration (most promising results)
- Use memory-building strategies in transitional neighborhoods
- Create "integration zones" with special support
- Monitor using our multi-metric framework

**Stakeholder Engagement**
- Different messages for different contexts
- Build coalitions based on shared interests
- Address specific concerns for each identity group
- Use data to counter misperceptions

**Adaptive Management**
- Regular monitoring using segregation metrics
- Adjust strategies based on observed patterns
- Learn from natural experiments
- Document and share successful interventions

These policy implications suggest that addressing residential segregation requires sophisticated, context-aware approaches that go beyond traditional fair housing tools. The dramatic differences between political, economic, and racial segregation patterns indicate that successful integration policies must be tailored to specific social contexts while building on universal human tendencies toward relationship formation and community building.

## AI Ethics and Bias Detection Implications

Our findings have profound implications for AI deployment and ethics:

### 1. Hidden Bias in "Neutral" Systems

LLMs marketed as unbiased may harbor context-dependent prejudices that:
- Emerge only in specific framings (political vs economic)
- Vary between models based on training data
- Operate through subtle associations rather than explicit rules
- May be inconsistent across contexts

### 2. The Measurement Challenge

Traditional bias testing may miss these contextual prejudices:
- Static benchmarks fail to capture dynamic, emergent biases
- Context-dependent biases require scenario-based testing
- Our framework offers a novel approach: using segregation models as bias detectors
- Different LLMs may exhibit different bias profiles

### 3. Dual-Use Nature of Biased LLMs

**For Research**: 
- Superior realism in modeling human social dynamics
- Reveal hidden biases in society through their outputs
- Enable study of prejudice without explicit programming
- Capture emergent behaviors mechanical models miss

**For Applications**:
- Risk perpetuating harmful stereotypes
- May make biased decisions in housing, lending, hiring
- Could reinforce segregation if used in urban planning
- Require careful monitoring and context-aware deployment

### 4. Toward Bias-Aware AI Deployment

Our findings suggest:
- Need for context-specific bias testing before deployment
- Importance of understanding training data demographics
- Value of comparative testing across multiple LLMs
- Potential for using bias as a diagnostic tool

## Limitations

1. **LLM-specific**: Results may vary with different models
2. **U.S.-centric**: Training data likely reflects American patterns
3. **Simplified identities**: No intersectionality or identity evolution
4. **Small scale**: 15×15 grid may not capture city-scale dynamics
5. **Single LLM tested**: Different models may show different bias patterns

## Future Directions

### Bias-Aware Agent-Based Modeling

Our findings point toward a new paradigm for ABM that leverages the bias paradox:

1. **Multi-LLM Ensemble Approaches**
   - Use multiple LLMs to capture bias uncertainty
   - Compare outputs across models to identify consistent vs variable biases
   - Develop "bias profiles" for different LLMs
   - Create confidence intervals based on model agreement

2. **Bias as Data**
   - Use LLM outputs to measure implicit societal biases
   - Track bias evolution by testing models trained on different time periods
   - Compare biases across cultures using locally-trained models
   - Develop bias indices for different social contexts

3. **Controlled Bias Studies**
   - Fine-tune LLMs on specific corpora to study bias formation
   - Remove or amplify certain biases to test interventions
   - Study how biases interact across multiple identities
   - Investigate bias transmission mechanisms

### Methodological Advances

1. **Dynamic Bias Detection**
   - Develop real-time bias monitoring for deployed LLMs
   - Create context-sensitive bias alerts
   - Build adaptive systems that adjust for detected biases
   - Design bias-aware decision support tools

2. **Intersectional Analysis**
   - Study compound biases (race + income + politics)
   - Examine bias hierarchies and interactions
   - Model code-switching and context-dependent identity
   - Investigate bias mitigation through intersectionality

3. **Cross-Model Validation**
   - Compare biases across different LLM architectures
   - Test consistency of social patterns
   - Identify model-specific vs universal biases
   - Develop bias robustness metrics

### Research Applications

1. **Social Science**
   - Use LLMs to study implicit bias without survey response bias
   - Model historical prejudice using period-trained models
   - Investigate emergence of new biases
   - Test interventions in virtual societies

2. **Policy Design**
   - Pre-test policies across different bias scenarios
   - Identify unintended consequences of well-meaning interventions
   - Design bias-resistant policies
   - Develop early warning systems for emerging prejudices

3. **AI Safety**
   - Create comprehensive bias testing frameworks
   - Develop bias mitigation strategies
   - Design transparent bias reporting standards
   - Build bias-aware AI governance structures

# Conclusion

This study reveals a fundamental paradox at the heart of Large Language Models: their greatest flaw—absorbing and perpetuating human biases—may also be their greatest strength for understanding human social dynamics.

## The Bias Paradox Demonstrated

Our experiments provide empirical evidence of this paradox. Without any explicit programming for discrimination, LLM agents reproduced segregation patterns that varied 12.3-fold between contexts, precisely mirroring real-world prejudices. Political contexts generated extreme segregation while economic contexts showed minimal clustering—patterns that emerged entirely from implicit associations in training data.

This demonstrates that LLMs are not neutral tools but carriers of human cultural biases, absorbed from the text they were trained on. They perpetuate stereotypes about who makes good neighbors, which groups are compatible, and how different identities interact—all without being explicitly taught these prejudices.

## Implications for Science and Society

**For Agent-Based Modeling**: The bias paradox makes LLMs superior to mechanical agents for studying social phenomena. Traditional models require researchers to explicitly program biases, potentially missing subtle prejudices or emergent dynamics. LLMs capture these automatically, revealing hidden patterns in human behavior that we might not even know to look for.

**For AI Deployment**: Our findings serve as a stark warning. LLMs deployed in housing, lending, hiring, or urban planning may perpetuate and amplify societal biases in unpredictable, context-dependent ways. The same model that shows economic tolerance might exhibit extreme political prejudice, making comprehensive, context-aware testing essential.

**For Policy Making**: The dramatic differences between contexts suggest one-size-fits-all integration policies will fail. Political segregation requires different interventions than racial segregation. Our framework enables virtual testing of policies before implementation, potentially avoiding costly failures.

## Toward Bias-Aware AI

Rather than viewing LLM biases purely as a problem to eliminate, we propose embracing the paradox:

1. **Leverage biases for research**: Use LLMs as mirrors of society, revealing implicit prejudices
2. **Develop bias-aware applications**: Design systems that account for and monitor contextual biases  
3. **Create bias detection tools**: Use frameworks like ours to measure hidden prejudices
4. **Build ensemble approaches**: Use multiple LLMs to capture bias uncertainty

The integration of LLMs into classic models like Schelling's opens new frontiers in computational social science. We can now ask not just "what if agents prefer similar neighbors?" but "what biases shape human preferences?"—and crucially, "how can we design interventions that work with, rather than against, the complex reality of human prejudice?"

As we stand at the intersection of AI advancement and social understanding, the bias paradox reminds us that our tools reflect ourselves—flaws and all. The question is not whether LLMs have biases, but how we can use this mirror wisely.

# References

::: {#refs}
:::

# Appendix: Implementation Details {.appendix}

## LLM Configuration
- Model: Qwen2.5-coder:32B
- Temperature: 0.7
- Max tokens: 500
- Batch processing: 50 agents in parallel

## Computational Requirements
- Mechanical agents: 0.02s per step
- LLM agents: 19.3s per step (~1000× slower)
- Memory overhead: ~50KB per agent with full history

## Convergence Detection
- Plateau detection: No metric changes for 10 consecutive steps
- Maximum steps: 1000 (most runs converge by step 200)